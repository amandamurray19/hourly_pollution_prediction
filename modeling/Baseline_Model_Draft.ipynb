{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc312edb-ff82-4420-bc6e-692b2df8f172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 19:00:53.479854: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-01 19:00:54.407090: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-08-01 19:00:54.407204: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-08-01 19:00:54.407212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import random\n",
    "from tensorflow.keras import layers, models, Input\n",
    "import tensorflow as tf\n",
    "\n",
    "folder = \"../../data/model_data/mini_data_set\"\n",
    "full_paths = [os.path.join(folder, f) for f in os.listdir(folder)]\n",
    "random.shuffle(full_paths)\n",
    "\n",
    "sensor_df = pd.read_csv(\"../../data/sensor_data/final_sensor_withgrid.csv\")\n",
    "sensor_df = sensor_df[['lat','lon']]\n",
    "sensor_df = sensor_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Shuffle the indices\n",
    "sensor_df = sensor_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Assign groups evenly\n",
    "sensor_df['cv_group'] = sensor_df.index % 5 + 1  # Groups: 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2351d87-f58e-41b3-8c46-32ee317f6f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ncDataLoader(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, img_paths, drop_sensor_locs_df):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_paths = img_paths\n",
    "        self.drop_sensor_locs_df = drop_sensor_locs_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.img_paths[i : i + self.batch_size]\n",
    "\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (22,), dtype=\"float32\")\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"float32\")\n",
    "\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            # print(path)\n",
    "            ds = xr.open_dataset(path)\n",
    "            input_ds = ds.copy()\n",
    "            input_ds = input_ds.drop_vars(\"y_sensor_no2\")\n",
    "            input_array = input_ds.to_array().values\n",
    "            x[j] = np.transpose(input_array, (1, 2, 0))\n",
    "            \n",
    "            target_ds = ds['y_sensor_no2']\n",
    "            \n",
    "            # Loop over rows to mask each (lat, lon) pair\n",
    "            for _, row in self.drop_sensor_locs_df.iterrows():\n",
    "                target_ds.loc[dict(lat=row['lat'], lon=row['lon'])] = np.nan\n",
    "            \n",
    "            # Replace NaNs with -999\n",
    "            target_ds = target_ds.values\n",
    "            # target_ds = np.where(np.isnan(target_ds), -999, target_ds)\n",
    "            y[j, :, :, 0] = target_ds\n",
    " \n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n",
    "def build_baseline_model(\n",
    "    input_shape=(479, 1059, 22)\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a convolution-free, per-pixel linear regression model.\n",
    "    Each pixel's 22 input features are combined linearly to produce 1 output value.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Input shape (H, W, C)\n",
    "\n",
    "    Returns:\n",
    "        Keras Model\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = layers.BatchNormalization()(inputs)\n",
    "\n",
    "    # Per-pixel linear regression (no hidden layer)\n",
    "    outputs = layers.Dense(1, activation='linear', use_bias=True)(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def masked_mse(y_true, y_pred):\n",
    "    # Create mask: True where y_true is not NaN\n",
    "    mask = tf.math.logical_not(tf.math.is_nan(y_true))\n",
    "    # tf.print(\"Valid pixels:\", tf.reduce_sum(tf.cast(mask, tf.int32)))\n",
    "\n",
    "    # Apply mask to both y_true and y_pred\n",
    "    y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "    \n",
    "    # Compute MSE only on valid values\n",
    "    diff_squared = tf.square(y_true_masked - y_pred_masked)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    denom = tf.cast(tf.size(diff_squared), tf.float32)\n",
    "    denom = tf.maximum(denom, 1e-6)\n",
    "\n",
    "    return tf.reduce_sum(diff_squared) / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be5d9d-1f5a-40eb-9cdb-5e2faf501349",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CV group 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 19:01:05.973845: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-08-01 19:01:05.973901: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-08-01 19:01:05.973930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sat-processing): /proc/driver/nvidia/version does not exist\n",
      "2025-08-01 19:01:05.974175: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "12/12 [==============================] - 153s 13s/step - loss: 15.9654 - val_loss: 2606.6765\n",
      "Epoch 2/4\n",
      "12/12 [==============================] - 151s 13s/step - loss: 9.6384 - val_loss: 1172.1293\n",
      "Epoch 3/4\n",
      "12/12 [==============================] - 151s 13s/step - loss: 5.8383 - val_loss: 381.0619\n",
      "Epoch 4/4\n",
      "12/12 [==============================] - 151s 13s/step - loss: 4.0560 - val_loss: 1463.2438\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Running CV group 2/5\n",
      "Epoch 1/4\n",
      "12/12 [==============================] - 153s 13s/step - loss: 12.8233 - val_loss: 5819.6055\n",
      "Epoch 2/4\n",
      "12/12 [==============================] - 152s 13s/step - loss: 6.7775 - val_loss: 3158.4729\n",
      "Epoch 3/4\n",
      "12/12 [==============================] - 154s 13s/step - loss: 4.6622 - val_loss: 6379.2046\n",
      "Epoch 4/4\n",
      "12/12 [==============================] - 153s 13s/step - loss: 3.8040 - val_loss: 4993.1206\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Running CV group 3/5\n",
      "Epoch 1/4\n",
      "12/12 [==============================] - 152s 13s/step - loss: 12.8773 - val_loss: 5479.7983\n",
      "Epoch 2/4\n",
      "12/12 [==============================] - 150s 13s/step - loss: 7.9126 - val_loss: 2854.2617\n",
      "Epoch 3/4\n",
      "12/12 [==============================] - 150s 13s/step - loss: 5.4146 - val_loss: 2656.1521\n",
      "Epoch 4/4\n",
      "12/12 [==============================] - 151s 13s/step - loss: 4.2600 - val_loss: 1557.7667\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Running CV group 4/5\n",
      "Epoch 1/4\n",
      "12/12 [==============================] - 151s 13s/step - loss: 12.8290 - val_loss: 6624.6567\n",
      "Epoch 2/4\n",
      "12/12 [==============================] - 150s 13s/step - loss: 7.2487 - val_loss: 2685.6885\n",
      "Epoch 3/4\n",
      "12/12 [==============================] - 150s 13s/step - loss: 4.5938 - val_loss: 2916.9153\n",
      "Epoch 4/4\n",
      "12/12 [==============================] - 150s 13s/step - loss: 3.7263 - val_loss: 2253.3301\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Running CV group 5/5\n",
      "Epoch 1/4\n",
      "12/12 [==============================] - 152s 13s/step - loss: 6.2663 - val_loss: 469.6040\n",
      "Epoch 2/4\n",
      "12/12 [==============================] - 152s 13s/step - loss: 2.4840 - val_loss: 649.6505\n",
      "Epoch 3/4\n",
      "12/12 [==============================] - 152s 13s/step - loss: 1.9631 - val_loss: 232.2050\n",
      "Epoch 4/4\n",
      " 1/12 [=>............................] - ETA: 1:56 - loss: 2.1970"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "##      Cross Validation and Save Results     ##\n",
    "################################################\n",
    "\n",
    "# Parameters\n",
    "batch_size = 4\n",
    "img_size = (479, 1059)\n",
    "img_paths = full_paths\n",
    "kernel_sizes_param = [3, 5, 3]\n",
    "CNN_filters_param = 64\n",
    "activation_param = 'relu'\n",
    "epochs_param = 4\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "for cv_group in range(1, 6):  # cv_group 1 to 5\n",
    "    print(f\"Running CV group {cv_group}/5\")\n",
    "    \n",
    "    drop_locs_train_set = sensor_df[sensor_df['cv_group'] == cv_group][['lat', 'lon']]\n",
    "    drop_locs_val_set = sensor_df[sensor_df['cv_group'] != cv_group][['lat', 'lon']]\n",
    "\n",
    "    train_loader = ncDataLoader(\n",
    "        batch_size=batch_size,\n",
    "        img_size=img_size,\n",
    "        img_paths=full_paths,\n",
    "        drop_sensor_locs_df=drop_locs_train_set\n",
    "    )\n",
    "\n",
    "    val_loader = ncDataLoader(\n",
    "        batch_size=batch_size,\n",
    "        img_size=img_size,\n",
    "        img_paths=full_paths,\n",
    "        drop_sensor_locs_df=drop_locs_val_set\n",
    "    )\n",
    "\n",
    "    # Reset model for each fold\n",
    "    model = build_baseline_model(input_shape=(479, 1059, 22))\n",
    "    model.compile(optimizer='adam', loss=masked_mse)\n",
    "\n",
    "    model.fit(\n",
    "        train_loader,\n",
    "        validation_data=val_loader,\n",
    "        epochs=epochs_param,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Predict and collect results\n",
    "    for i, (batch_x, batch_y_true) in enumerate(val_loader):\n",
    "        batch_y_pred = model.predict(batch_x)\n",
    "\n",
    "        for j in range(len(batch_y_true)):\n",
    "            y_true = batch_y_true[j]\n",
    "            y_pred = batch_y_pred[j]\n",
    "\n",
    "            mask = ~np.isnan(y_true)\n",
    "            rows, cols, _ = np.where(mask)\n",
    "\n",
    "            true_vals = y_true[rows, cols]\n",
    "            pred_vals = y_pred[rows, cols]\n",
    "\n",
    "            # Optional: use corresponding file name for traceability\n",
    "            file_idx = i * batch_size + j\n",
    "            file_name = os.path.basename(full_paths[file_idx]) if file_idx < len(full_paths) else None\n",
    "\n",
    "            for r, c, t, p in zip(rows, cols, true_vals, pred_vals):\n",
    "                cv_results.append({\n",
    "                    'cv_group': cv_group,\n",
    "                    'file': file_name,\n",
    "                    'row': r,\n",
    "                    'col': c,\n",
    "                    'true': t[0],\n",
    "                    'predicted': p[0],\n",
    "                    'lat': r * 0.01 + 28.605,\n",
    "                    'lon': c * 0.01 -98.895\n",
    "                })\n",
    "\n",
    "df_cv = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b5279be-aefc-4de4-91a1-1267a06e89cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cv.to_csv('test_model_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "172ceb32-2fa2-478e-b96b-d93bc4cae421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sensor_df.to_csv('cv_groups_and_lat_lon.csv')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
