{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4293de77-54f1-42dc-9491-dc0a125d5190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from bbox import * \n",
    "from pytz import timezone\n",
    "import os\n",
    "from shapely.wkt import loads\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1325908-7ba0-4b3c-a195-7729912fd62e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(f'../../data/sensor_data/full_gridded_sensoronly.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7520c41-da2f-41fe-85a6-965a79448db6",
   "metadata": {},
   "source": [
    "# Sensor Data Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac9fec20-f445-4b4c-ab19-34e10e37b334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sensor_df=pd.read_csv('../../data/sensor_data/hourly_raw_am.csv', index_col=0)\n",
    "sensor_df=sensor_df[['latitude', 'longitude', 'date_local',\n",
    "       'time_local', 'sample_measurement', 'site_number', 'county','state']]\n",
    "sensor_df=sensor_df[~sensor_df['sample_measurement'].isna()]\n",
    "sensor_df['time'] = pd.to_datetime(sensor_df['date_local'] + ' ' + sensor_df['time_local'])\n",
    "\n",
    "\n",
    "no2_hours = pd.read_csv(\"../../data/tempo_data/no2_file_hours.csv\")\n",
    "no2_hours=list(no2_hours['time_hr_ct'])\n",
    "time_list = np.array(no2_hours, dtype=\"datetime64[ns]\")\n",
    "sensor_time_list = np.array(sensor_df['time'].unique(), dtype=\"datetime64[ns]\")\n",
    "seta = set(sensor_time_list)\n",
    "setb = set(time_list)\n",
    "union_set = seta & setb\n",
    "full_time_list = list(union_set)\n",
    "# Convert to pandas datetime objects\n",
    "pd_datetime_list = list(pd.to_datetime(full_time_list))\n",
    "sensor_df['time']=pd.to_datetime(sensor_df['time'])\n",
    "sensor_df=sensor_df[sensor_df['time'].isin(pd_datetime_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e62fe0b-a073-4613-83b3-9f12d49e5ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_27677/1432961727.py:25: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  grid_gdf[\"lat\"] = grid_gdf.geometry.centroid.y\n",
      "/var/tmp/ipykernel_27677/1432961727.py:26: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  grid_gdf[\"lon\"] = grid_gdf.geometry.centroid.x\n"
     ]
    }
   ],
   "source": [
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    sensor_df,\n",
    "    geometry=[Point(xy) for xy in zip(sensor_df[\"longitude\"], sensor_df[\"latitude\"])],\n",
    "    crs=\"EPSG:4326\"  # WGS84 coordinate reference system\n",
    ")\n",
    "gdf=gdf[['latitude','longitude','geometry']].drop_duplicates()\n",
    "\n",
    "# Create gridded GDF\n",
    "res = 0.01 \n",
    "lon_bins = np.arange(lon_min+0.005, lon_max, res)\n",
    "lat_bins = np.arange(lat_min+0.005, lat_max, res)\n",
    "\n",
    "grid_cells = []\n",
    "for x in lon_bins[:-1]:\n",
    "    for y in lat_bins[:-1]:\n",
    "        grid_cells.append(\n",
    "            Point(x, y).buffer(res / 2).envelope\n",
    "        )\n",
    "\n",
    "grid_gdf = gpd.GeoDataFrame(\n",
    "    {\"geometry\": grid_cells},\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "grid_gdf[\"lat\"] = grid_gdf.geometry.centroid.y\n",
    "grid_gdf[\"lon\"] = grid_gdf.geometry.centroid.x\n",
    "\n",
    "key = gpd.sjoin(grid_gdf, gdf, how=\"inner\", predicate=\"intersects\")\n",
    "key=key[['geometry','lat','lon','latitude','longitude']]\n",
    "\n",
    "sensor_df_joined = pd.merge(sensor_df, key, on=['latitude','longitude'], how='left')\n",
    "# sensor_df_joined = sensor_df_joined[['lat', 'lon', 'time', 'sample_measurement']]\n",
    "sensor_df_joined=sensor_df_joined.rename(columns={'sample_measurement':'no2'})\n",
    "\n",
    "del gdf\n",
    "del sensor_df\n",
    "del grid_cells\n",
    "del key\n",
    "gc.collect()\n",
    "\n",
    "sensor_df_joined.to_csv('../../data/sensor_data/final_sensor_withgrid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe23863-4fc1-4f1d-85a7-cc69b0c22bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'h5netcdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Concatenate all the xarrays in the list along the 'time' dimension\u001b[39;00m\n\u001b[1;32m     29\u001b[0m final_ds \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mconcat(xarrays_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mfinal_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/sensor_data/full_gridded_sensoronly.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh5netcdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ds_env/lib/python3.11/site-packages/xarray/core/dataarray.py:4168\u001b[0m, in \u001b[0;36mDataArray.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf, auto_complex)\u001b[0m\n\u001b[1;32m   4164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4165\u001b[0m     \u001b[38;5;66;03m# No problems with the name - so we're fine!\u001b[39;00m\n\u001b[1;32m   4166\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dataset()\n\u001b[0;32m-> 4168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[1;32m   4169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4176\u001b[0m \u001b[43m    \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultifile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4179\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4181\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ds_env/lib/python3.11/site-packages/xarray/backends/api.py:1855\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf, auto_complex)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_complex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1853\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_complex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m auto_complex\n\u001b[0;32m-> 1855\u001b[0m store \u001b[38;5;241m=\u001b[39m \u001b[43mstore_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unlimited_dims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1858\u001b[0m     unlimited_dims \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mencoding\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munlimited_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/ds_env/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py:154\u001b[0m, in \u001b[0;36mH5NetCDFStore.open\u001b[0;34m(cls, filename, mode, format, group, lock, autoclose, invalid_netcdf, phony_dims, decode_vlen_strings, driver, driver_kwds)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     driver_kwds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    153\u001b[0m ):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5netcdf\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    158\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt open netCDF4/HDF5 as bytes \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtry passing a path or file-like object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'h5netcdf'"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold xarrays\n",
    "xarrays_list = []\n",
    "\n",
    "for i, time in enumerate(pd_datetime_list):\n",
    "    if i % 500 == 0:\n",
    "        print(i)\n",
    "    no2_df = sensor_df_joined[sensor_df_joined['time']==time]\n",
    "    time_df = pd.merge(grid_gdf, no2_df, on=['lat', 'lon'], how='left')\n",
    "    time_df['time'] = time_df['time'].fillna(time)\n",
    "    \n",
    "    # Pivot the data into a 2D array\n",
    "    time_df_pivot = time_df.pivot(index=\"lat\", columns=\"lon\", values=\"no2\")\n",
    "\n",
    "    # Convert to xarray\n",
    "    da = xr.DataArray(\n",
    "        time_df_pivot.values,\n",
    "        coords={\n",
    "            \"lat\": time_df_pivot.index.values,\n",
    "            \"lon\": time_df_pivot.columns.values,\n",
    "        },\n",
    "        dims=[\"lat\", \"lon\"],\n",
    "        name=\"no2\"\n",
    "    )\n",
    "    ds_expanded = da.expand_dims({'time': [time]})\n",
    "    # Append the expanded xarray to the list\n",
    "    xarrays_list.append(ds_expanded)\n",
    "\n",
    "# Concatenate all the xarrays in the list along the 'time' dimension\n",
    "final_ds = xr.concat(xarrays_list, dim='time')\n",
    "\n",
    "final_ds.to_netcdf(f'../../data/sensor_data/full_gridded_sensoronly.nc', engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf924a3-4b47-48c2-a231-6f82a48e94a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_ds.to_netcdf(f'../../data/sensor_data/full_gridded_sensoronly.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ae2e2-073d-4dd3-9a39-3c24d371852c",
   "metadata": {},
   "source": [
    "# Sensor and Mobile Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a477b-76d8-4184-afe6-51604e63d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df_joined=pd.read_csv('../../data/sensor_data/final_sensor_withgrid.csv')\n",
    "mobile_df = pd.read_csv('../../data/mobile_air_data/final_sensor_withgrid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71189f74-6535-42e8-ac2b-538dfaa53149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initialize an empty list to hold xarrays\n",
    "# xarrays_list = []\n",
    "\n",
    "# for i, time in enumerate(pd_datetime_list):\n",
    "#     print(i)\n",
    "#     no2_df = final_df[final_df['time']==time]\n",
    "#     time_df = pd.merge(grid_gdf, no2_df, on=['geometry', 'lat', 'lon'], how='left')\n",
    "#     time_df['time'] = time_df['time'].fillna(time)\n",
    "    \n",
    "#     # Pivot the data into a 2D array\n",
    "#     time_df_pivot = time_df.pivot(index=\"lat\", columns=\"lon\", values=\"weighted_no2\")\n",
    "\n",
    "#     # Convert to xarray\n",
    "#     da = xr.DataArray(\n",
    "#         time_df_pivot.values,\n",
    "#         coords={\n",
    "#             \"lat\": time_df_pivot.index.values,\n",
    "#             \"lon\": time_df_pivot.columns.values,\n",
    "#         },\n",
    "#         dims=[\"lat\", \"lon\"],\n",
    "#         name=\"no2\"\n",
    "#     )\n",
    "#     ds_expanded = da.expand_dims({'time': [time]})\n",
    "#     # Append the expanded xarray to the list\n",
    "#     xarrays_list.append(ds_expanded)\n",
    "\n",
    "# # Concatenate all the xarrays in the list along the 'time' dimension\n",
    "# final_ds = xr.concat(xarrays_list, dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffa2d2-8323-4a43-8f30-ac4e6ba9a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_ds.to_netcdf(f'../../data/mobile_air_data/full_gridded_mobile.nc', engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4670f9f-a21a-46ab-8fea-aff59715d25f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# # # Define your latitude and longitude bounds\n",
    "# # lat_min, lat_max = 28.6, 33.4  # Example latitude range\n",
    "# # lon_min, lon_max = -98.9, -88.3  # Example longitude range\n",
    "\n",
    "# # Select one hour of temperature data (e.g., the first timestamp)\n",
    "# hour_index = 1  # Change this to select a different hour if desired\n",
    "# # temperature_data = nldas_month['temperature'].isel(time=hour_index)\n",
    "# temperature_data = nldas_month['wind_u']\n",
    "\n",
    "# Plot the data with switched axes\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot with latitude on x-axis and longitude on y-axis\n",
    "final_ds[175].plot(\n",
    "    cmap=\"Reds\",  # Colormap for temperature visualization\n",
    "    cbar_kwargs={'label': 'Temperature (K)'}  # Add color bar label\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Longitude\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "ds_env",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python (ds_env) (Local)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
