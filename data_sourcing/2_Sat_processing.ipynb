{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e1fb91-dab6-4593-ba2f-8a2f0b14f398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".netrc file does not exist.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import xesmf as xe\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import base64\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import platform\n",
    "from subprocess import Popen\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "from load_credentials import *\n",
    "from bbox import * \n",
    "from netCDF4 import Dataset\n",
    "import netCDF4 as nc\n",
    "from bbox import * \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define event with credentials and S3 endpoint details\n",
    "event = {\n",
    "    's3_endpoint': 'https://data.asdc.earthdata.nasa.gov/s3credentials',  # replace with actual endpoint\n",
    "    'edl_username': 'amanda.murray19',  # replace with your EDL username\n",
    "    'edl_password': 'Sat_modeling_berk2024',  # replace with your EDL password\n",
    "    'bucket_name': 'asdc-prod-protected/TEMPO/TEMPO_NO2_L3_V03'  # replace with your bucket name\n",
    "}\n",
    "\n",
    "netrc_path = os.path.expanduser('~/.netrc')  # Expands to the user's home directory\n",
    "if os.path.exists(netrc_path):\n",
    "    print(\".netrc file exists.\")\n",
    "    os.remove(netrc_path)\n",
    "    print(\".netrc file has been removed.\")\n",
    "else:\n",
    "    print(\".netrc file does not exist.\")# Retrieve credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98267084-81ec-4359-91a3-56e85fd29001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/tempo_data/no2_tempo_files_df.csv\")\n",
    "df['time_hr_ct'] = pd.to_datetime(df['timestamp_ct']).dt.floor('h')\n",
    "\n",
    "result_df = df.groupby(['date_central', 'Hour'], as_index=False).first()\n",
    "result_df['time_hr_ct'] = pd.to_datetime(result_df['timestamp_ct']).dt.floor('h')\n",
    "result_df[['time_hr_ct']].to_csv('../../data/tempo_data/no2_file_hours.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43896aff-197d-48ee-b5f6-33e91b7576b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [307]>\n",
      "<Response [307]>\n",
      "<Response [307]>\n",
      "<Response [307]>\n",
      "<Response [307]>\n",
      "<Response [307]>\n"
     ]
    }
   ],
   "source": [
    "### TIME 5 days = 875 s (~15 min)\n",
    "# Initialize the Google Cloud Storage client\n",
    "# Example DataFrame (assuming it exists)\n",
    "satellite_null_data = pd.DataFrame({\n",
    "    'date': [],\n",
    "    'file': [],\n",
    "    'time': [],\n",
    "    'null_count': [],\n",
    "    'percent_null': []\n",
    "})\n",
    "\n",
    "date_list = list(df['date_central'].unique())\n",
    "\n",
    "for date in date_list[25:54]:\n",
    "    # Retrieve credentials\n",
    "    creds = retrieve_credentials(event)\n",
    "\n",
    "    # Use the credentials to access the S3 bucket\n",
    "    client = boto3.client('s3',\n",
    "        aws_access_key_id=creds[\"accessKeyId\"],\n",
    "        aws_secret_access_key=creds[\"secretAccessKey\"],\n",
    "        aws_session_token=creds[\"sessionToken\"]\n",
    "    )\n",
    "    \n",
    "    file_df = df[df['date_central']==date]\n",
    "    filepaths = list(file_df['FilePath'])\n",
    "    times = list(file_df['time_hr_ct'])\n",
    "    full_time = list(file_df['timestamp_ct'])\n",
    "    \n",
    "    daily_xr_list = []\n",
    "    \n",
    "    for i in range(len(filepaths)):\n",
    "        file=filepaths[i]\n",
    "        new_time = str(np.array(times[i]))\n",
    "        \n",
    "        bucket_name = 'asdc-prod-protected'\n",
    "        object_key = file\n",
    "        local_file_name = 'current_sat.nc'\n",
    "        client.download_file(bucket_name, object_key, local_file_name)\n",
    "        \n",
    "        ds = nc.Dataset('current_sat.nc', mode='r')\n",
    "        support_data_xr = ds.groups['support_data']\n",
    "        support_data_xr = xr.open_dataset(xr.backends.NetCDF4DataStore(support_data_xr))\n",
    "        support_data_xr=support_data_xr[['vertical_column_total','eff_cloud_fraction']]\n",
    "        tempo = xr.open_dataset('current_sat.nc', group=\"/product\")\n",
    "        tempo = xr.merge([tempo, support_data_xr])\n",
    "        del ds\n",
    "        del support_data_xr\n",
    "        lat_lon_coords = xr.open_dataset('current_sat.nc')\n",
    "        lat = lat_lon_coords.coords['latitude'].values\n",
    "        lon = lat_lon_coords.coords['longitude'].values\n",
    "        tempo = tempo.assign_coords(latitude=(\"latitude\", lat), longitude=(\"longitude\", lon), time=[new_time])\n",
    "        del lat\n",
    "        del lon\n",
    "\n",
    "        # Filter the data using the .sel method for selecting ranges in xarray\n",
    "        louisiana_data = tempo.sel(latitude=(tempo.latitude  >= lat_min) & (tempo.latitude <= lat_max))\n",
    "        louisiana_data = louisiana_data.sel(longitude=(tempo.longitude  >= lon_min) & (tempo.longitude <= lon_max))\n",
    "        del tempo\n",
    "        \n",
    "        mask = louisiana_data['main_data_quality_flag'] >1 \n",
    "        louisiana_data = louisiana_data.where(~mask, other=np.nan)\n",
    "        mask = louisiana_data['main_data_quality_flag'].isnull()\n",
    "        louisiana_data = louisiana_data.where(~mask, other=np.nan)\n",
    "        \n",
    "        lat_new = np.arange(lat_min+0.005, lat_max, 0.01)\n",
    "        lon_new = np.arange(lon_min+0.005, lon_max, 0.01)\n",
    "\n",
    "        ds_out = xr.Dataset(\n",
    "            {\n",
    "                \"lat\": ([\"lat\"], lat_new),\n",
    "                \"lon\": ([\"lon\"], lon_new),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Create the regridder object.\n",
    "        regridder = xe.Regridder(louisiana_data, ds_out, \"bilinear\")\n",
    "\n",
    "        # Apply the regridding operation.\n",
    "        louisiana_data_regrid = regridder(louisiana_data)\n",
    "        daily_xr_list.append(louisiana_data_regrid)\n",
    "        \n",
    "        null_count = int(louisiana_data_regrid.vertical_column_troposphere.isnull().sum().values)\n",
    "        total_elements = louisiana_data_regrid.vertical_column_troposphere.size\n",
    "        nan_percentage = round((null_count / total_elements) * 100,0)\n",
    "        \n",
    "        new_row = {'date':date, 'file':file, 'time':new_time, 'real_time':full_time, 'null_count':null_count, 'percent_null':nan_percentage}\n",
    "        # Convert the new row to a DataFrame\n",
    "        new_row_df = pd.DataFrame([new_row])\n",
    "        # Append the new row to the DataFrame using concat\n",
    "        satellite_null_data = pd.concat([satellite_null_data, new_row_df], ignore_index=True)\n",
    "        \n",
    "    combined_data = xr.concat(daily_xr_list, dim='time')\n",
    "    \n",
    "    combined_data.to_netcdf(f'../../data/tempo_data/daily_files/tempo_{date}.nc')\n",
    "         \n",
    "\n",
    "satellite_null_data.to_csv(\"../../data/tempo_data/satellite_null_data_09_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d049e4-8e7f-48ed-b0cb-b2af26f27070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### TIME 5 days = 875 s (~15 min)\n",
    "# Initialize the Google Cloud Storage client\n",
    "# Example DataFrame (assuming it exists)\n",
    "satellite_null_data = pd.DataFrame({\n",
    "    'date': [],\n",
    "    'file': [],\n",
    "    'time': [],\n",
    "    'null_count': [],\n",
    "    'percent_null': []\n",
    "})\n",
    "\n",
    "date_list = list(df['date_central'].unique())\n",
    "\n",
    "for date in date_list[54:85]:\n",
    "    # Retrieve credentials\n",
    "    creds = retrieve_credentials(event)\n",
    "\n",
    "    # Use the credentials to access the S3 bucket\n",
    "    client = boto3.client('s3',\n",
    "        aws_access_key_id=creds[\"accessKeyId\"],\n",
    "        aws_secret_access_key=creds[\"secretAccessKey\"],\n",
    "        aws_session_token=creds[\"sessionToken\"]\n",
    "    )\n",
    "    \n",
    "    file_df = df[df['date_central']==date]\n",
    "    filepaths = list(file_df['FilePath'])\n",
    "    times = list(file_df['time_hr_ct'])\n",
    "    full_time = list(file_df['timestamp_ct'])\n",
    "    \n",
    "    daily_xr_list = []\n",
    "    \n",
    "    for i in range(len(filepaths)):\n",
    "        file=filepaths[i]\n",
    "        new_time = str(np.array(times[i]))\n",
    "        \n",
    "        bucket_name = 'asdc-prod-protected'\n",
    "        object_key = file\n",
    "        local_file_name = 'current_sat.nc'\n",
    "        client.download_file(bucket_name, object_key, local_file_name)\n",
    "        \n",
    "        ds = nc.Dataset('current_sat.nc', mode='r')\n",
    "        support_data_xr = ds.groups['support_data']\n",
    "        support_data_xr = xr.open_dataset(xr.backends.NetCDF4DataStore(support_data_xr))\n",
    "        support_data_xr=support_data_xr[['vertical_column_total','eff_cloud_fraction']]\n",
    "        tempo = xr.open_dataset('current_sat.nc', group=\"/product\")\n",
    "        tempo = xr.merge([tempo, support_data_xr])\n",
    "        del ds\n",
    "        del support_data_xr\n",
    "        lat_lon_coords = xr.open_dataset('current_sat.nc')\n",
    "        lat = lat_lon_coords.coords['latitude'].values\n",
    "        lon = lat_lon_coords.coords['longitude'].values\n",
    "        tempo = tempo.assign_coords(latitude=(\"latitude\", lat), longitude=(\"longitude\", lon), time=[new_time])\n",
    "        del lat\n",
    "        del lon\n",
    "\n",
    "        # Filter the data using the .sel method for selecting ranges in xarray\n",
    "        louisiana_data = tempo.sel(latitude=(tempo.latitude  >= lat_min) & (tempo.latitude <= lat_max))\n",
    "        louisiana_data = louisiana_data.sel(longitude=(tempo.longitude  >= lon_min) & (tempo.longitude <= lon_max))\n",
    "        del tempo\n",
    "        \n",
    "        mask = louisiana_data['main_data_quality_flag'] >1 \n",
    "        louisiana_data = louisiana_data.where(~mask, other=np.nan)\n",
    "        mask = louisiana_data['main_data_quality_flag'].isnull()\n",
    "        louisiana_data = louisiana_data.where(~mask, other=np.nan)\n",
    "        \n",
    "        lat_new = np.arange(lat_min+0.005, lat_max, 0.01)\n",
    "        lon_new = np.arange(lon_min+0.005, lon_max, 0.01)\n",
    "\n",
    "        ds_out = xr.Dataset(\n",
    "            {\n",
    "                \"lat\": ([\"lat\"], lat_new),\n",
    "                \"lon\": ([\"lon\"], lon_new),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Create the regridder object.\n",
    "        regridder = xe.Regridder(louisiana_data, ds_out, \"bilinear\")\n",
    "\n",
    "        # Apply the regridding operation.\n",
    "        louisiana_data_regrid = regridder(louisiana_data)\n",
    "        daily_xr_list.append(louisiana_data_regrid)\n",
    "        \n",
    "        null_count = int(louisiana_data_regrid.vertical_column_troposphere.isnull().sum().values)\n",
    "        total_elements = louisiana_data_regrid.vertical_column_troposphere.size\n",
    "        nan_percentage = round((null_count / total_elements) * 100,0)\n",
    "        \n",
    "        new_row = {'date':date, 'file':file, 'time':new_time, 'real_time':full_time, 'null_count':null_count, 'percent_null':nan_percentage}\n",
    "        # Convert the new row to a DataFrame\n",
    "        new_row_df = pd.DataFrame([new_row])\n",
    "        # Append the new row to the DataFrame using concat\n",
    "        satellite_null_data = pd.concat([satellite_null_data, new_row_df], ignore_index=True)\n",
    "        \n",
    "    combined_data = xr.concat(daily_xr_list, dim='time')\n",
    "    \n",
    "    combined_data.to_netcdf(f'../../data/tempo_data/daily_files/tempo_{date}.nc')\n",
    "         \n",
    "\n",
    "satellite_null_data.to_csv(\"../../data/tempo_data/satellite_null_data_10_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953890c8-c921-4303-873e-33d36993e649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [307]>\n",
      "<Response [307]>\n",
      "<Response [307]>\n"
     ]
    }
   ],
   "source": [
    "### TIME 5 days = 875 s (~15 min)\n",
    "# Initialize the Google Cloud Storage client\n",
    "# Example DataFrame (assuming it exists)\n",
    "satellite_null_data = pd.DataFrame({\n",
    "    'date': [],\n",
    "    'file': [],\n",
    "    'time': [],\n",
    "    'null_count': [],\n",
    "    'percent_null': []\n",
    "})\n",
    "\n",
    "date_list = list(df['date_central'].unique())\n",
    "\n",
    "for date in date_list[259:290]:\n",
    "    # Retrieve credentials\n",
    "    creds = retrieve_credentials(event)\n",
    "\n",
    "    # Use the credentials to access the S3 bucket\n",
    "    client = boto3.client('s3',\n",
    "        aws_access_key_id=creds[\"accessKeyId\"],\n",
    "        aws_secret_access_key=creds[\"secretAccessKey\"],\n",
    "        aws_session_token=creds[\"sessionToken\"]\n",
    "    )\n",
    "    \n",
    "    file_df = df[df['date_central']==date]\n",
    "    filepaths = list(file_df['FilePath'])\n",
    "    times = list(file_df['time_hr_ct'])\n",
    "    full_time = list(file_df['timestamp_ct'])\n",
    "    \n",
    "    daily_xr_list = []\n",
    "    \n",
    "    for i in range(len(filepaths)):\n",
    "        file=filepaths[i]\n",
    "        new_time = str(np.array(times[i]))\n",
    "        \n",
    "        bucket_name = 'asdc-prod-protected'\n",
    "        object_key = file\n",
    "        local_file_name = 'current_sat.nc'\n",
    "        client.download_file(bucket_name, object_key, local_file_name)\n",
    "        \n",
    "        ds = nc.Dataset('current_sat.nc', mode='r')\n",
    "        support_data_xr = ds.groups['support_data']\n",
    "        support_data_xr = xr.open_dataset(xr.backends.NetCDF4DataStore(support_data_xr))\n",
    "        support_data_xr=support_data_xr[['vertical_column_total','eff_cloud_fraction']]\n",
    "        tempo = xr.open_dataset('current_sat.nc', group=\"/product\")\n",
    "        tempo = xr.merge([tempo, support_data_xr])\n",
    "        del ds\n",
    "        del support_data_xr\n",
    "        lat_lon_coords = xr.open_dataset('current_sat.nc')\n",
    "        lat = lat_lon_coords.coords['latitude'].values\n",
    "        lon = lat_lon_coords.coords['longitude'].values\n",
    "        tempo = tempo.assign_coords(latitude=(\"latitude\", lat), longitude=(\"longitude\", lon), time=[new_time])\n",
    "        del lat\n",
    "        del lon\n",
    "\n",
    "        # Filter the data using the .sel method for selecting ranges in xarray\n",
    "        louisiana_data = tempo.sel(latitude=(tempo.latitude  >= lat_min) & (tempo.latitude <= lat_max))\n",
    "        louisiana_data = louisiana_data.sel(longitude=(tempo.longitude  >= lon_min) & (tempo.longitude <= lon_max))\n",
    "        del tempo\n",
    "        \n",
    "        mask = louisiana_data['main_data_quality_flag'] >1 \n",
    "        louisiana_data = louisiana_data.where(~mask, other=np.nan)\n",
    "        mask = louisiana_data['main_data_quality_flag'].isnull()\n",
    "        louisiana_data = louisiana_data.where(~mask, other=np.nan)\n",
    "        \n",
    "        lat_new = np.arange(lat_min+0.005, lat_max, 0.01)\n",
    "        lon_new = np.arange(lon_min+0.005, lon_max, 0.01)\n",
    "\n",
    "        ds_out = xr.Dataset(\n",
    "            {\n",
    "                \"lat\": ([\"lat\"], lat_new),\n",
    "                \"lon\": ([\"lon\"], lon_new),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Create the regridder object.\n",
    "        regridder = xe.Regridder(louisiana_data, ds_out, \"bilinear\")\n",
    "\n",
    "        # Apply the regridding operation.\n",
    "        louisiana_data_regrid = regridder(louisiana_data)\n",
    "        daily_xr_list.append(louisiana_data_regrid)\n",
    "        \n",
    "        null_count = int(louisiana_data_regrid.vertical_column_troposphere.isnull().sum().values)\n",
    "        total_elements = louisiana_data_regrid.vertical_column_troposphere.size\n",
    "        nan_percentage = round((null_count / total_elements) * 100,0)\n",
    "        \n",
    "        new_row = {'date':date, 'file':file, 'time':new_time, 'real_time':full_time, 'null_count':null_count, 'percent_null':nan_percentage}\n",
    "        # Convert the new row to a DataFrame\n",
    "        new_row_df = pd.DataFrame([new_row])\n",
    "        # Append the new row to the DataFrame using concat\n",
    "        satellite_null_data = pd.concat([satellite_null_data, new_row_df], ignore_index=True)\n",
    "        \n",
    "    combined_data = xr.concat(daily_xr_list, dim='time')\n",
    "    \n",
    "    combined_data.to_netcdf(f'../../data/tempo_data/daily_files/tempo_{date}.nc')\n",
    "         \n",
    "\n",
    "satellite_null_data.to_csv(\"../../data/tempo_data/satellite_null_data_05_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff40249-6917-4537-8420-91da9847a3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # Open the file with xarray\n",
    "# ds = xr.open_dataset('daily.nc')\n",
    "# # Assuming your xarray is called 'x'\n",
    "# # Define the bounding box for Louisiana\n",
    "# lat_min, lat_max = 28.92, 33.02\n",
    "# lon_min, lon_max = -94.04, -88.82\n",
    "\n",
    "# subset = ds.sel(time='2023-08-07 07:00:00')\n",
    "\n",
    "# # Plot the weight variable in Louisiana's bounding box\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# subset.vertical_column_troposphere.plot(cmap='viridis')\n",
    "# plt.title('Weight Variable in Louisiana Bounding Box')\n",
    "# plt.xlabel('Longitude')\n",
    "# plt.ylabel('Latitude')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "ds_env",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python (ds_env) (Local)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
