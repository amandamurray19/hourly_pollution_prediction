{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e1fb91-dab6-4593-ba2f-8a2f0b14f398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".netrc file does not exist.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import base64\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import platform\n",
    "from subprocess import Popen\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "from load_credentials import *\n",
    "from bbox import * \n",
    "from netCDF4 import Dataset\n",
    "import netCDF4 as nc\n",
    "from bbox import * \n",
    "import numpy as np\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "\n",
    "# Define event with credentials and S3 endpoint details\n",
    "event = {\n",
    "    's3_endpoint': 'https://data.asdc.earthdata.nasa.gov/s3credentials',  # replace with actual endpoint\n",
    "    'edl_username': 'amanda.murray19',  # replace with your EDL username\n",
    "    'edl_password': 'Sat_modeling_berk2024',  # replace with your EDL password\n",
    "    'bucket_name': 'asdc-prod-protected/TEMPO/TEMPO_O3TOT_L3_V03'  # replace with your bucket name\n",
    "}\n",
    "\n",
    "netrc_path = os.path.expanduser('~/.netrc')  # Expands to the user's home directory\n",
    "if os.path.exists(netrc_path):\n",
    "    print(\".netrc file exists.\")\n",
    "    os.remove(netrc_path)\n",
    "    print(\".netrc file has been removed.\")\n",
    "else:\n",
    "    print(\".netrc file does not exist.\")# Retrieve credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98267084-81ec-4359-91a3-56e85fd29001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/tempo_data/o3_tempo_files_df.csv\")\n",
    "df['time_hr_ct'] = pd.to_datetime(df['timestamp_ct']).dt.floor('h')\n",
    "\n",
    "result_df = df.groupby(['date_central', 'Hour'], as_index=False).first()\n",
    "result_df['time_hr_ct'] = pd.to_datetime(result_df['timestamp_ct']).dt.floor('h')\n",
    "result_df[['time_hr_ct']].to_csv('../../data/tempo_data/o3_file_hours.csv')\n",
    "\n",
    "df['month_year'] = pd.to_datetime(df['date_central']).dt.to_period('M')\n",
    "df['year'] = pd.to_datetime(df['date_central']).dt.year\n",
    "date_groups = df.groupby('month_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fbdf01b-b200-410f-962a-aa43968e91c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-01\n",
      "2025-05-02\n",
      "2025-05-03\n",
      "2025-05-04\n",
      "2025-05-05\n",
      "2025-05-06\n",
      "2025-05-07\n",
      "2025-05-08\n",
      "2025-05-09\n",
      "2025-05-10\n",
      "2025-05-11\n",
      "2025-05-12\n",
      "2025-05-13\n",
      "2025-05-14\n",
      "2025-05-15\n",
      "2025-05-16\n",
      "2025-05-17\n",
      "2025-05-18\n",
      "2025-05-19\n",
      "2025-05-20\n",
      "2025-05-21\n",
      "2025-05-22\n",
      "2025-05-23\n",
      "2025-05-24\n",
      "2025-05-25\n",
      "2025-05-26\n",
      "2025-05-27\n",
      "2025-05-28\n",
      "2025-05-29\n",
      "2025-05-30\n",
      "2025-05-31\n"
     ]
    }
   ],
   "source": [
    "for mo in df[(df['month_year']=='2025-05')]['date_central'].unique():\n",
    "    print(mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea61bde-ba83-48d8-8761-d2736d27fe57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_2025 = df[(df['month_year']=='2024-12')]\n",
    "date_groups = df_2025.groupby('month_year')\n",
    "\n",
    "# Set up a Dask distributed cluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "# Function to process a single month/year group\n",
    "@delayed\n",
    "def process_group(group_name, group_df):\n",
    "    local_null_data = []\n",
    "    date_list = group_df['date_central'].unique()\n",
    "\n",
    "    for date in date_list:\n",
    "        creds = retrieve_credentials(event)\n",
    "        \n",
    "        client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=creds[\"accessKeyId\"],\n",
    "            aws_secret_access_key=creds[\"secretAccessKey\"],\n",
    "            aws_session_token=creds[\"sessionToken\"]\n",
    "        )\n",
    "        file_df = group_df[group_df['date_central'] == date]\n",
    "        filepaths = list(file_df['FilePath'])\n",
    "        full_time = list(file_df['timestamp_ct'])\n",
    "        daily_xr_list = []\n",
    "        \n",
    "        for i in range(len(filepaths)):\n",
    "            file = filepaths[i]\n",
    "            new_time = str(np.array(full_time[i]))\n",
    "\n",
    "            bucket_name = 'asdc-prod-protected'\n",
    "            object_key = file\n",
    "            local_file_name = f'current_sat_o3_{group_name}.nc'\n",
    "            client.download_file(bucket_name, object_key, local_file_name)\n",
    "\n",
    "            # Processing the NetCDF file\n",
    "            ds = nc.Dataset(local_file_name, mode='r')\n",
    "            \n",
    "            tempo = xr.open_dataset(f'current_sat_o3_{group_name}.nc', group=\"/product\")\n",
    "            lat_lon_coords = xr.open_dataset(f'current_sat_o3_{group_name}.nc')\n",
    "            lat = lat_lon_coords.coords['latitude'].values\n",
    "            lon = lat_lon_coords.coords['longitude'].values\n",
    "            tempo = tempo.assign_coords(latitude=(\"latitude\", lat), longitude=(\"longitude\", lon), time=[new_time])\n",
    "            louisiana_data = tempo.sel(latitude=(tempo.latitude  >= lat_min) & (tempo.latitude <= lat_max))\n",
    "            louisiana_data = louisiana_data.sel(longitude=(tempo.longitude  >= lon_min) & (tempo.longitude <= lon_max))\n",
    "            # louisiana_data = louisiana_data.where(~(louisiana_data['main_data_quality_flag'] > 1), np.nan)\n",
    "            # lat_new = np.arange(lat_min + 0.005, lat_max, 0.01)\n",
    "            # lon_new = np.arange(lon_min + 0.005, lon_max, 0.01)\n",
    "            # ds_out = xr.Dataset({\"lat\": ([\"lat\"], lat_new), \"lon\": ([\"lon\"], lon_new)})\n",
    "            # regridder = xe.Regridder(louisiana_data, ds_out, \"bilinear\")\n",
    "            # louisiana_data_regrid = regridder(louisiana_data)\n",
    "            daily_xr_list.append(louisiana_data)\n",
    "\n",
    "            # Null count and percentage\n",
    "            null_count = int(louisiana_data.column_amount_o3.isnull().sum().values)\n",
    "            total_elements = louisiana_data.column_amount_o3.size\n",
    "            nan_percentage = round((null_count / total_elements) * 100,0)\n",
    "            nan_percentage = round((null_count / total_elements) * 100, 0)\n",
    "            new_row = {'date': date, 'file': file, 'time': new_time, 'real_time': full_time, \n",
    "                       'null_count': null_count, 'percent_null': nan_percentage}\n",
    "            local_null_data.append(new_row)\n",
    "\n",
    "        combined_data = xr.concat(daily_xr_list, dim='time')\n",
    "        combined_data.to_netcdf(f'../../data/tempo_data/o3_daily_files/tempo_o3_{date}.nc')\n",
    "\n",
    "    # Save satellite_null_data for the month/year\n",
    "    local_null_df = pd.DataFrame(local_null_data)\n",
    "    local_null_df.to_csv(f\"../../data/tempo_data/satellite_null_data_raw_o3_{group_name}.csv\", index=False)\n",
    "    return f\"Completed processing for {group_name}\"\n",
    "\n",
    "# Retrieve credentials\n",
    "# creds = retrieve_credentials(event)\n",
    "\n",
    "# Submit tasks to Dask\n",
    "tasks = [process_group(group_name, group_df) for group_name, group_df in date_groups]\n",
    "\n",
    "# Execute tasks\n",
    "results = compute(*tasks)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847038b1-d599-4872-b4e1-097390f87648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TIME 5 days = 875 s (~15 min)\n",
    "# # Initialize the Google Cloud Storage client\n",
    "# # Example DataFrame (assuming it exists)\n",
    "# satellite_null_data = pd.DataFrame({\n",
    "#     'date': [],\n",
    "#     'file': [],\n",
    "#     'time': [],\n",
    "#     'null_count': [],\n",
    "#     'percent_null': []\n",
    "# })\n",
    "\n",
    "# date_list = list(df['date_central'].unique())\n",
    "\n",
    "# for date in date_list:\n",
    "#     # Retrieve credentials\n",
    "#     creds = retrieve_credentials(event)\n",
    "\n",
    "#     # Use the credentials to access the S3 bucket\n",
    "#     client = boto3.client('s3',\n",
    "#         aws_access_key_id=creds[\"accessKeyId\"],\n",
    "#         aws_secret_access_key=creds[\"secretAccessKey\"],\n",
    "#         aws_session_token=creds[\"sessionToken\"]\n",
    "#     )\n",
    "    \n",
    "#     file_df = df[df['date_central']==date]\n",
    "#     filepaths = list(file_df['FilePath'])\n",
    "#     times = list(file_df['time_hr_ct'])\n",
    "#     full_time = list(file_df['timestamp_ct'])\n",
    "    \n",
    "#     daily_xr_list = []\n",
    "    \n",
    "#     for i in range(len(filepaths)):\n",
    "#         file=filepaths[i]\n",
    "#         new_time = str(np.array(times[i]))\n",
    "        \n",
    "#         bucket_name = 'asdc-prod-protected'\n",
    "#         object_key = file\n",
    "#         local_file_name = 'current_sat_o3.nc'\n",
    "#         client.download_file(bucket_name, object_key, local_file_name)\n",
    "        \n",
    "#         tempo = xr.open_dataset('current_sat_o3.nc', group=\"/product\")\n",
    "#         lat_lon_coords = xr.open_dataset('current_sat_o3.nc')\n",
    "#         lat = lat_lon_coords.coords['latitude'].values\n",
    "#         lon = lat_lon_coords.coords['longitude'].values\n",
    "#         tempo = tempo.assign_coords(latitude=(\"latitude\", lat), longitude=(\"longitude\", lon), time=[new_time])\n",
    "#         louisiana_data = tempo.sel(latitude=(tempo.latitude  >= lat_min) & (tempo.latitude <= lat_max))\n",
    "#         louisiana_data = louisiana_data.sel(longitude=(tempo.longitude  >= lon_min) & (tempo.longitude <= lon_max))\n",
    "#         del tempo\n",
    "#         del lat\n",
    "#         del lon\n",
    "        \n",
    "#         lat_new = np.arange(lat_min+0.005, lat_max, 0.01)\n",
    "#         lon_new = np.arange(lon_min+0.005, lon_max, 0.01)\n",
    "\n",
    "#         ds_out = xr.Dataset(\n",
    "#             {\n",
    "#                 \"lat\": ([\"lat\"], lat_new),\n",
    "#                 \"lon\": ([\"lon\"], lon_new),\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#         # Create the regridder object.\n",
    "#         regridder = xe.Regridder(louisiana_data, ds_out, \"bilinear\")\n",
    "\n",
    "#         # Apply the regridding operation.\n",
    "#         louisiana_data_regrid = regridder(louisiana_data)\n",
    "#         daily_xr_list.append(louisiana_data_regrid)\n",
    "        \n",
    "#         null_count = int(louisiana_data_regrid.column_amount_o3.isnull().sum().values)\n",
    "#         total_elements = louisiana_data_regrid.column_amount_o3.size\n",
    "#         nan_percentage = round((null_count / total_elements) * 100,0)\n",
    "        \n",
    "#         new_row = {'date':date, 'file':file, 'time':new_time, 'real_time':full_time, 'null_count':null_count, 'percent_null':nan_percentage}\n",
    "#         # Convert the new row to a DataFrame\n",
    "#         new_row_df = pd.DataFrame([new_row])\n",
    "#         # Append the new row to the DataFrame using concat\n",
    "#         satellite_null_data = pd.concat([satellite_null_data, new_row_df], ignore_index=True)\n",
    "        \n",
    "#     combined_data = xr.concat(daily_xr_list, dim='time')\n",
    "    \n",
    "#     combined_data.to_netcdf(f'../../data/tempo_data/o3_daily_files/tempo_o3_{date}.nc')\n",
    "         \n",
    "\n",
    "# satellite_null_data.to_csv(\"../../data/tempo_data/o3_null_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a5413-3bd6-4103-b496-67b3821e1c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # # Define your latitude and longitude bounds\n",
    "# # lat_min, lat_max = 28.6, 33.4  # Example latitude range\n",
    "# # lon_min, lon_max = -98.9, -88.3  # Example longitude range\n",
    "\n",
    "# # Select one hour of temperature data (e.g., the first timestamp)\n",
    "# # hour_index = 1  # Change this to select a different hour if desired\n",
    "# temperature_data = louisiana_data['uv_aerosol_index']\n",
    "# # temperature_data = ds['Percent_Tree_Cover']\n",
    "\n",
    "# # Plot the data with switched axes\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot with latitude on x-axis and longitude on y-axis\n",
    "# temperature_data.T.plot(\n",
    "#     cmap=\"coolwarm\",  # Colormap for temperature visualization\n",
    "#     cbar_kwargs={'label': 'Temperature (K)'}  # Add color bar label\n",
    "# )\n",
    "\n",
    "# # Update axis labels\n",
    "# plt.xlabel(\"Latitude\")\n",
    "# plt.ylabel(\"Longitude\")\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "ds_env",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python (ds_env) (Local)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
