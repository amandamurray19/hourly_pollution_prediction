{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c8444e-6e5c-4ca4-afac-3f8d7c6ee5d5",
   "metadata": {},
   "source": [
    "# Load Filenames and Times for TEMPO Data\n",
    "\n",
    "This notebook:\n",
    "1) Pulls the filenames for all available L3 files from s3\n",
    "2) Pulls the filenames for all L2 files for swaths that cross over the bounding box\n",
    "3) Map L2 to L3 files\n",
    "4) Calculate L3 times by taking the midpoint of the swath times (halfway between beginning of first swath and end of last swath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e1fb91-dab6-4593-ba2f-8a2f0b14f398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import xesmf as xe\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import base64\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import platform\n",
    "from subprocess import Popen\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import earthaccess\n",
    "from load_credentials import *\n",
    "from bbox import *\n",
    "\n",
    "# Define event with credentials and S3 endpoint details\n",
    "event = {\n",
    "    's3_endpoint': 'https://data.asdc.earthdata.nasa.gov/s3credentials',  # replace with actual endpoint\n",
    "    'edl_username': 'amanda.murray19',  # replace with your EDL username\n",
    "    'edl_password': 'Sat_modeling_berk2024',  # replace with your EDL password\n",
    "    'bucket_name': 'asdc-prod-protected/TEMPO/TEMPO_O3TOT_L3_V03'  # replace with your bucket name\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e49c3-75d5-4752-9670-ab21b950fb36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get all L3 file names from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca19782d-e70b-4a59-bd70-acc6c7dfb195",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".netrc file does not exist.\n",
      "<Response [307]>\n"
     ]
    }
   ],
   "source": [
    "# Remove .netrc file if it exists\n",
    "netrc_path = os.path.expanduser('~/.netrc')  # Expands to the user's home directory\n",
    "if os.path.exists(netrc_path):\n",
    "    print(\".netrc file exists.\")\n",
    "    os.remove(netrc_path)\n",
    "    print(\".netrc file has been removed.\")\n",
    "else:\n",
    "    print(\".netrc file does not exist.\")# Retrieve credentials\n",
    "\n",
    "creds = retrieve_credentials(event)\n",
    "\n",
    "# Use the credentials to access the S3 bucket\n",
    "client = boto3.client('s3',\n",
    "    aws_access_key_id=creds[\"accessKeyId\"],\n",
    "    aws_secret_access_key=creds[\"secretAccessKey\"],\n",
    "    aws_session_token=creds[\"sessionToken\"])\n",
    "\n",
    "bucket_name = 'asdc-prod-protected'\n",
    "prefix = 'TEMPO/TEMPO_O3TOT_L3_V03/'\n",
    "\n",
    "client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "# List objects in the bucket\n",
    "response = client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "tempo_l3_files = []\n",
    "\n",
    "# Loop to handle pagination if there are more than 1000 objects\n",
    "while response.get('Contents'):\n",
    "    # Append the object keys to the list\n",
    "    tempo_l3_files.extend([r[\"Key\"] for r in response['Contents']])\n",
    "\n",
    "    # Check if there is a next page of results\n",
    "    if response.get('NextContinuationToken'):\n",
    "        response = client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, ContinuationToken=response['NextContinuationToken'])\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Pull only L3 files\n",
    "tempo_l3_files_nc = []\n",
    "\n",
    "for file_name in tempo_l3_files:\n",
    "    if file_name.endswith('.nc'):\n",
    "        tempo_l3_files_nc.append(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448693d-c9fc-4d03-8a2e-9114abb8edc4",
   "metadata": {},
   "source": [
    "## Get all L3 file names from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98267084-81ec-4359-91a3-56e85fd29001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write files to a text file\n",
    "with open(\"/home/jupyter/.netrc\", \"w\") as f:\n",
    "   f.write(\"\"\"machine urs.earthdata.nasa.gov\n",
    "login amanda.murray19\n",
    "password Sat_modeling_berk2024\"\"\")\n",
    "\n",
    "# Establishing access to EarthData,\n",
    "auth = earthaccess.login(strategy=\"netrc\", persist=True)\n",
    "\n",
    "# Get current date and time\n",
    "current_datetime = datetime.now()\n",
    "formatted_current_datetime = current_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#Set up collection information along with timeframe and region of interest\n",
    "short_name = 'TEMPO_O3TOT_L2' # collection name to search for in the EarthData\n",
    "date_start = '2022-05-01 00:00:00' # start date\n",
    "date_end = formatted_current_datetime # end date\n",
    "bbox = (lon_min, lat_min, lon_max, lat_max) # 1 degree bounding box around POI\n",
    "\n",
    "# search for granules\n",
    "results = earthaccess.search_data(short_name = short_name\\\n",
    ", temporal = (date_start, date_end)\n",
    ", bounding_box = bbox)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9af659-5e13-4fd2-bc1c-4545a93b6643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_pattern = re.compile(r\"TEMPO/TEMPO_O3TOT_L2_V\\d+/[\\d.]+/TEMPO_O3TOT_L2_V\\d+_\\d+T\\d+Z_S\\d+G\\d+\\.nc\")\n",
    "datetime_pattern = re.compile(r'BeginningDateTime\\': \\'([^\\']+)\\', \\'EndingDateTime\\': \\'([^\\']+)')\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate through your results\n",
    "for result in results:  # Adjust `results[:4]` as needed\n",
    "    files = file_pattern.findall(str(result))\n",
    "    datetimes = datetime_pattern.findall(str(result))\n",
    "\n",
    "    # Ensure both files and datetimes have matches before proceeding\n",
    "    if files and datetimes:\n",
    "        for file, (begin, end) in zip(files, datetimes):\n",
    "            # Append each record as a tuple (or dictionary)\n",
    "            data.append({\n",
    "                \"L2_File\": file,\n",
    "                \"BeginningDateTime\": begin,\n",
    "                \"EndingDateTime\": end\n",
    "            })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert BeginningDateTime and EndingDateTime columns to timestamps\n",
    "df['BeginningDateTime'] = pd.to_datetime(df['BeginningDateTime'])\n",
    "df['EndingDateTime'] = pd.to_datetime(df['EndingDateTime'])\n",
    "\n",
    "# Extract the date (20230809) and file number (S011) using regular expressions\n",
    "df['Date'] = df['L2_File'].str.extract(r'_(\\d{8})T')\n",
    "df['FileNumber'] = df['L2_File'].str.extract(r'_(S\\d+)')\n",
    "\n",
    "# Set beginning_time as the time of first swath and ending_time as end of last swath\n",
    "df_grouped = df.groupby(['Date', 'FileNumber']).agg({'L2_File':'count', 'BeginningDateTime':'min','EndingDateTime':'max'}).reset_index()\n",
    "# Make Time as the midpoint\n",
    "df_grouped['Time'] = df_grouped['BeginningDateTime']+(df_grouped['EndingDateTime'] - df_grouped['BeginningDateTime'])/2\n",
    "df_grouped['Time'] = df_grouped['Time'].round('S')\n",
    "\n",
    "# Create a DataFrame from the list of lines\n",
    "df_L3 = pd.DataFrame(tempo_l3_files_nc, columns=['FilePath'])\n",
    "df_L3['Date'] = df_L3['FilePath'].str.extract(r'_(\\d{8})T')\n",
    "df_L3['FileNumber'] = df_L3['FilePath'].str.extract(r'_(S\\d+)')\n",
    "\n",
    "# Full dataset\n",
    "full_dataset = pd.merge(df_L3, df_grouped, on=['Date', 'FileNumber'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3cd782-e66c-4bff-94e6-b18427a1a1bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to central\n",
    "full_dataset['timestamp_ct'] = full_dataset['Time'].dt.tz_convert('America/Chicago')\n",
    "full_dataset['date_central'] = full_dataset['timestamp_ct'].dt.date\n",
    "\n",
    "# Remove the timezone part from the string (keeping only the date and time)\n",
    "full_dataset['timestamp_ct'] = full_dataset['timestamp_ct'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "full_dataset['timestamp_ct'] = full_dataset['timestamp_ct'].str[:19]\n",
    "\n",
    "# Convert the 'timestamp_ct' column to datetime\n",
    "full_dataset['timestamp_ct'] = pd.to_datetime(full_dataset['timestamp_ct'])\n",
    "full_dataset['Hour'] = full_dataset['timestamp_ct'].dt.hour\n",
    "full_dataset['Minute'] = full_dataset['timestamp_ct'].dt.minute\n",
    "full_dataset =full_dataset.sort_values('timestamp_ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4dc68-c72a-41ff-b43c-ef30442fda15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_dataset.to_csv(\"../../data/tempo_data/o3_tempo_files_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf91c7b-f1de-4ab5-a7f4-03f8576f929d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "ds_env",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python (ds_env) (Local)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
