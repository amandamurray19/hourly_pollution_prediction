{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e1fb91-dab6-4593-ba2f-8a2f0b14f398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".netrc file does not exist.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import xesmf as xe\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import base64\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import platform\n",
    "from subprocess import Popen\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "from load_credentials import *\n",
    "from bbox import * \n",
    "from netCDF4 import Dataset\n",
    "import netCDF4 as nc\n",
    "from bbox import * \n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Define event with credentials and S3 endpoint details\n",
    "event = {\n",
    "    's3_endpoint': 'https://data.asdc.earthdata.nasa.gov/s3credentials',  # replace with actual endpoint\n",
    "    'edl_username': 'amanda.murray19',  # replace with your EDL username\n",
    "    'edl_password': 'Sat_modeling_berk2024',  # replace with your EDL password\n",
    "    'bucket_name': 'asdc-prod-protected/TEMPO/TEMPO_NO2_L3_V03'  # replace with your bucket name\n",
    "}\n",
    "\n",
    "netrc_path = os.path.expanduser('~/.netrc')  # Expands to the user's home directory\n",
    "if os.path.exists(netrc_path):\n",
    "    print(\".netrc file exists.\")\n",
    "    os.remove(netrc_path)\n",
    "    print(\".netrc file has been removed.\")\n",
    "else:\n",
    "    print(\".netrc file does not exist.\")# Retrieve credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98267084-81ec-4359-91a3-56e85fd29001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/tempo_data/no2_tempo_files_df.csv\")\n",
    "df['time_hr_ct'] = pd.to_datetime(df['timestamp_ct']).dt.floor('h')\n",
    "\n",
    "result_df = df.groupby(['date_central', 'Hour'], as_index=False).first()\n",
    "result_df['time_hr_ct'] = pd.to_datetime(result_df['timestamp_ct']).dt.floor('h')\n",
    "result_df[['time_hr_ct']].to_csv('../../data/tempo_data/no2_file_hours.csv')\n",
    "\n",
    "df['month_year'] = pd.to_datetime(df['date_central']).dt.to_period('M')\n",
    "df['year'] = pd.to_datetime(df['date_central']).dt.year\n",
    "date_groups = df.groupby('month_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50125dde-2632-4904-b893-db376882fc4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:59:31,400 - distributed.scheduler - WARNING - Worker failed to heartbeat for 494s; attempting restart: <WorkerState 'tcp://127.0.0.1:40045', name: 3, status: running, memory: 0, processing: 2>\n",
      "2025-06-29 15:59:35,405 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-06-29 15:59:36,405 - distributed.core - ERROR - Exception while handling op kill\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/utils.py\", line 1910, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/core.py\", line 834, in _handle_comm\n",
      "    result = await result\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/nanny.py\", line 400, in kill\n",
      "    await self.process.kill(reason=reason, timeout=timeout)\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/nanny.py\", line 883, in kill\n",
      "    await process.join(max(0, deadline - time()))\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/process.py\", line 330, in join\n",
      "    await wait_for(asyncio.shield(self._exit_future), timeout)\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/utils.py\", line 1909, in wait_for\n",
      "    async with asyncio.timeout(timeout):\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/asyncio/timeouts.py\", line 115, in __aexit__\n",
      "    raise TimeoutError from exc_val\n",
      "TimeoutError\n",
      "2025-06-29 15:59:36,520 - distributed.scheduler - ERROR - Workers ['tcp://127.0.0.1:40045'] did not shut down within 30s; force closing\n",
      "2025-06-29 15:59:36,554 - distributed.scheduler - ERROR - 1/1 nanny worker(s) did not shut down within 30s: {'tcp://127.0.0.1:40045'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/utils.py\", line 805, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/scheduler.py\", line 6609, in restart_workers\n",
      "    raise TimeoutError(\n",
      "TimeoutError: 1/1 nanny worker(s) did not shut down within 30s: {'tcp://127.0.0.1:40045'}\n",
      "2025-06-29 15:59:36,556 - distributed.scheduler - ERROR - 1/1 nanny worker(s) did not shut down within 30s: {'tcp://127.0.0.1:40045'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/utils.py\", line 805, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/scheduler.py\", line 8648, in check_worker_ttl\n",
      "    await self.restart_workers(\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/utils.py\", line 805, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/scheduler.py\", line 6609, in restart_workers\n",
      "    raise TimeoutError(\n",
      "TimeoutError: 1/1 nanny worker(s) did not shut down within 30s: {'tcp://127.0.0.1:40045'}\n",
      "2025-06-29 15:59:36,558 - tornado.application - ERROR - Exception in callback <bound method Scheduler.check_worker_ttl of <Scheduler 'tcp://127.0.0.1:36593', workers: 3, cores: 12, tasks: 6>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/tornado/ioloop.py\", line 939, in _run\n",
      "    await val\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/utils.py\", line 805, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/scheduler.py\", line 8648, in check_worker_ttl\n",
      "    await self.restart_workers(\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/utils.py\", line 805, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/ds_env/lib/python3.11/site-packages/distributed/scheduler.py\", line 6609, in restart_workers\n",
      "    raise TimeoutError(\n",
      "TimeoutError: 1/1 nanny worker(s) did not shut down within 30s: {'tcp://127.0.0.1:40045'}\n",
      "2025-06-29 15:59:36,582 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Completed processing for 2025-01', 'Completed processing for 2025-02', 'Completed processing for 2025-03', 'Completed processing for 2025-04', 'Completed processing for 2025-05', 'Completed processing for 2025-06')\n"
     ]
    }
   ],
   "source": [
    "df_2025 = df[df['year']==2025]\n",
    "date_groups = df_2025.groupby('month_year')\n",
    "\n",
    "# Set up a Dask distributed cluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "# Function to process a single month/year group\n",
    "@delayed\n",
    "def process_group(group_name, group_df):\n",
    "    local_null_data = []\n",
    "    date_list = group_df['date_central'].unique()\n",
    "\n",
    "    for date in date_list:\n",
    "        creds = retrieve_credentials(event)\n",
    "        \n",
    "        client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=creds[\"accessKeyId\"],\n",
    "            aws_secret_access_key=creds[\"secretAccessKey\"],\n",
    "            aws_session_token=creds[\"sessionToken\"]\n",
    "        )\n",
    "        file_df = group_df[group_df['date_central'] == date]\n",
    "        filepaths = list(file_df['FilePath'])\n",
    "        full_time = list(file_df['timestamp_ct'])\n",
    "        daily_xr_list = []\n",
    "        \n",
    "        for i in range(len(filepaths)):\n",
    "            file = filepaths[i]\n",
    "            new_time = str(np.array(full_time[i]))\n",
    "\n",
    "            bucket_name = 'asdc-prod-protected'\n",
    "            object_key = file\n",
    "            local_file_name = f'current_sat_{group_name}.nc'\n",
    "            client.download_file(bucket_name, object_key, local_file_name)\n",
    "\n",
    "            # Processing the NetCDF file\n",
    "            ds = nc.Dataset(local_file_name, mode='r')\n",
    "            support_data_xr = ds.groups['support_data']\n",
    "            support_data_xr = xr.open_dataset(xr.backends.NetCDF4DataStore(support_data_xr))\n",
    "            support_data_xr = support_data_xr[['vertical_column_total', 'eff_cloud_fraction']]\n",
    "            tempo = xr.open_dataset(local_file_name, group=\"/product\")\n",
    "            tempo = xr.merge([tempo, support_data_xr])\n",
    "            lat_lon_coords = xr.open_dataset(local_file_name)\n",
    "            lat = lat_lon_coords.coords['latitude'].values\n",
    "            lon = lat_lon_coords.coords['longitude'].values\n",
    "            tempo = tempo.assign_coords(latitude=(\"latitude\", lat), longitude=(\"longitude\", lon), time=[new_time])\n",
    "\n",
    "            # Filtering and regridding\n",
    "            louisiana_data = tempo.sel(\n",
    "                latitude=(tempo.latitude >= lat_min) & (tempo.latitude <= lat_max),\n",
    "                longitude=(tempo.longitude >= lon_min) & (tempo.longitude <= lon_max)\n",
    "            )\n",
    "            louisiana_data = louisiana_data.where(~(louisiana_data['main_data_quality_flag'] > 1), np.nan)\n",
    "            # lat_new = np.arange(lat_min + 0.005, lat_max, 0.01)\n",
    "            # lon_new = np.arange(lon_min + 0.005, lon_max, 0.01)\n",
    "            # ds_out = xr.Dataset({\"lat\": ([\"lat\"], lat_new), \"lon\": ([\"lon\"], lon_new)})\n",
    "            # regridder = xe.Regridder(louisiana_data, ds_out, \"bilinear\")\n",
    "            # louisiana_data_regrid = regridder(louisiana_data)\n",
    "            daily_xr_list.append(louisiana_data)\n",
    "\n",
    "            # Null count and percentage\n",
    "            null_count = int(louisiana_data.vertical_column_total.isnull().sum().values)\n",
    "            total_elements = louisiana_data.vertical_column_total.size\n",
    "            nan_percentage = round((null_count / total_elements) * 100, 0)\n",
    "            new_row = {'date': date, 'file': file, 'time': new_time, 'real_time': full_time, \n",
    "                       'null_count': null_count, 'percent_null': nan_percentage}\n",
    "            local_null_data.append(new_row)\n",
    "\n",
    "        combined_data = xr.concat(daily_xr_list, dim='time')\n",
    "        combined_data.to_netcdf(f'../../data/tempo_data/no2_daily_files/tempo_{date}.nc')\n",
    "\n",
    "    # Save satellite_null_data for the month/year\n",
    "    local_null_df = pd.DataFrame(local_null_data)\n",
    "    local_null_df.to_csv(f\"../../data/tempo_data/satellite_null_data_raw_{group_name}.csv\", index=False)\n",
    "    return f\"Completed processing for {group_name}\"\n",
    "\n",
    "# Retrieve credentials\n",
    "# creds = retrieve_credentials(event)\n",
    "\n",
    "# Submit tasks to Dask\n",
    "tasks = [process_group(group_name, group_df) for group_name, group_df in date_groups]\n",
    "\n",
    "# Execute tasks\n",
    "results = compute(*tasks)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4074d255-f49f-429e-b5a4-fecab5c1c8d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### WHEN RERUN, SPLIT UP INTO GROUPS \n",
    "df_2024 = df[df['year']==2024]\n",
    "df_2024 = df_2024[(df_2024['month_year'].astype(str)=='2024-11')|(df_2024['month_year'].astype(str)=='2024-12')]\n",
    "# df_2024 = df_2024[df_2024['month_year'].astype(str)!='2024-01']\n",
    "# df_2024 = df_2024[df_2024['month_year'].astype(str)!='2024-04']\n",
    "# df_2024 = df_2024[df_2024['month_year'].astype(str)!='2024-05']\n",
    "# df_2024 = df_2024[df_2024['month_year'].astype(str)!='2024-11']\n",
    "\n",
    "date_groups = df_2024.groupby('month_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0233cdac-3237-4825-b7c5-dddaaa2831eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>FilePath</th>\n",
       "      <th>Date</th>\n",
       "      <th>FileNumber</th>\n",
       "      <th>L2_File</th>\n",
       "      <th>BeginningDateTime</th>\n",
       "      <th>EndingDateTime</th>\n",
       "      <th>Time</th>\n",
       "      <th>timestamp_ct</th>\n",
       "      <th>date_central</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>time_hr_ct</th>\n",
       "      <th>month_year</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6225</th>\n",
       "      <td>6225</td>\n",
       "      <td>TEMPO/TEMPO_NO2_L3_V03/2024.11.01/TEMPO_NO2_L3...</td>\n",
       "      <td>20241101</td>\n",
       "      <td>S002</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-11-01 12:41:41+00:00</td>\n",
       "      <td>2024-11-01 13:01:32+00:00</td>\n",
       "      <td>2024-11-01 12:51:36.500000+00:00</td>\n",
       "      <td>2024-11-01 07:51:36</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>7</td>\n",
       "      <td>51</td>\n",
       "      <td>2024-11-01 07:00:00</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6226</th>\n",
       "      <td>6226</td>\n",
       "      <td>TEMPO/TEMPO_NO2_L3_V03/2024.11.01/TEMPO_NO2_L3...</td>\n",
       "      <td>20241101</td>\n",
       "      <td>S003</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-11-01 13:21:46+00:00</td>\n",
       "      <td>2024-11-01 13:41:37+00:00</td>\n",
       "      <td>2024-11-01 13:31:41.500000+00:00</td>\n",
       "      <td>2024-11-01 08:31:41</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>2024-11-01 08:00:00</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6227</th>\n",
       "      <td>6227</td>\n",
       "      <td>TEMPO/TEMPO_NO2_L3_V03/2024.11.01/TEMPO_NO2_L3...</td>\n",
       "      <td>20241101</td>\n",
       "      <td>S004</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-11-01 14:01:51+00:00</td>\n",
       "      <td>2024-11-01 14:21:42+00:00</td>\n",
       "      <td>2024-11-01 14:11:46.500000+00:00</td>\n",
       "      <td>2024-11-01 09:11:46</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>2024-11-01 09:00:00</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6228</th>\n",
       "      <td>6228</td>\n",
       "      <td>TEMPO/TEMPO_NO2_L3_V03/2024.11.01/TEMPO_NO2_L3...</td>\n",
       "      <td>20241101</td>\n",
       "      <td>S005</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-11-01 14:41:59+00:00</td>\n",
       "      <td>2024-11-01 15:01:50+00:00</td>\n",
       "      <td>2024-11-01 14:51:54.500000+00:00</td>\n",
       "      <td>2024-11-01 09:51:54</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>9</td>\n",
       "      <td>51</td>\n",
       "      <td>2024-11-01 09:00:00</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6229</th>\n",
       "      <td>6229</td>\n",
       "      <td>TEMPO/TEMPO_NO2_L3_V03/2024.11.01/TEMPO_NO2_L3...</td>\n",
       "      <td>20241101</td>\n",
       "      <td>S006</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-11-01 15:41:59+00:00</td>\n",
       "      <td>2024-11-01 16:01:50+00:00</td>\n",
       "      <td>2024-11-01 15:51:54.500000+00:00</td>\n",
       "      <td>2024-11-01 10:51:54</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>10</td>\n",
       "      <td>51</td>\n",
       "      <td>2024-11-01 10:00:00</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>6988</td>\n",
       "      <td>TEMPO/TEMPO_NO2_L3_V03/2024.12.31/TEMPO_NO2_L3...</td>\n",
       "      <td>20241231</td>\n",
       "      <td>S008</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-12-31 18:07:55+00:00</td>\n",
       "      <td>2024-12-31 18:27:46+00:00</td>\n",
       "      <td>2024-12-31 18:17:50.500000+00:00</td>\n",
       "      <td>2024-12-31 12:17:50</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>2024-12-31 12:00:00</td>\n",
       "      <td>2024-12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>6989</td>\n",
       "      <td>TEMPO/TEMPO_NO2_L3_V03/2024.12.31/TEMPO_NO2_L3...</td>\n",
       "      <td>20241231</td>\n",
       "      <td>S009</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-12-31 19:07:55+00:00</td>\n",
       "      <td>2024-12-31 19:27:46+00:00</td>\n",
       "      <td>2024-12-31 19:17:50.500000+00:00</td>\n",
       "      <td>2024-12-31 13:17:50</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>2024-12-31 13:00:00</td>\n",
       "      <td>2024-12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990</th>\n",
       "      <td>6990</td>\n",
       "      <td>TEMPO/TEMPO_NO2_L3_V03/2024.12.31/TEMPO_NO2_L3...</td>\n",
       "      <td>20241231</td>\n",
       "      <td>S010</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-12-31 20:07:55+00:00</td>\n",
       "      <td>2024-12-31 20:27:46+00:00</td>\n",
       "      <td>2024-12-31 20:17:50.500000+00:00</td>\n",
       "      <td>2024-12-31 14:17:50</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>2024-12-31 14:00:00</td>\n",
       "      <td>2024-12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6991</th>\n",
       "      <td>6991</td>\n",
       "      <td>TEMPO/TEMPO_NO2_L3_V03/2024.12.31/TEMPO_NO2_L3...</td>\n",
       "      <td>20241231</td>\n",
       "      <td>S011</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-12-31 21:07:55+00:00</td>\n",
       "      <td>2024-12-31 21:27:46+00:00</td>\n",
       "      <td>2024-12-31 21:17:50.500000+00:00</td>\n",
       "      <td>2024-12-31 15:17:50</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>2024-12-31 15:00:00</td>\n",
       "      <td>2024-12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6992</th>\n",
       "      <td>6992</td>\n",
       "      <td>TEMPO/TEMPO_NO2_L3_V03/2024.12.31/TEMPO_NO2_L3...</td>\n",
       "      <td>20241231</td>\n",
       "      <td>S012</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-12-31 21:47:58+00:00</td>\n",
       "      <td>2024-12-31 22:07:19+00:00</td>\n",
       "      <td>2024-12-31 21:57:38.500000+00:00</td>\n",
       "      <td>2024-12-31 15:57:38</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>15</td>\n",
       "      <td>57</td>\n",
       "      <td>2024-12-31 15:00:00</td>\n",
       "      <td>2024-12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                           FilePath      Date  \\\n",
       "6225        6225  TEMPO/TEMPO_NO2_L3_V03/2024.11.01/TEMPO_NO2_L3...  20241101   \n",
       "6226        6226  TEMPO/TEMPO_NO2_L3_V03/2024.11.01/TEMPO_NO2_L3...  20241101   \n",
       "6227        6227  TEMPO/TEMPO_NO2_L3_V03/2024.11.01/TEMPO_NO2_L3...  20241101   \n",
       "6228        6228  TEMPO/TEMPO_NO2_L3_V03/2024.11.01/TEMPO_NO2_L3...  20241101   \n",
       "6229        6229  TEMPO/TEMPO_NO2_L3_V03/2024.11.01/TEMPO_NO2_L3...  20241101   \n",
       "...          ...                                                ...       ...   \n",
       "6988        6988  TEMPO/TEMPO_NO2_L3_V03/2024.12.31/TEMPO_NO2_L3...  20241231   \n",
       "6989        6989  TEMPO/TEMPO_NO2_L3_V03/2024.12.31/TEMPO_NO2_L3...  20241231   \n",
       "6990        6990  TEMPO/TEMPO_NO2_L3_V03/2024.12.31/TEMPO_NO2_L3...  20241231   \n",
       "6991        6991  TEMPO/TEMPO_NO2_L3_V03/2024.12.31/TEMPO_NO2_L3...  20241231   \n",
       "6992        6992  TEMPO/TEMPO_NO2_L3_V03/2024.12.31/TEMPO_NO2_L3...  20241231   \n",
       "\n",
       "     FileNumber  L2_File          BeginningDateTime  \\\n",
       "6225       S002        3  2024-11-01 12:41:41+00:00   \n",
       "6226       S003        3  2024-11-01 13:21:46+00:00   \n",
       "6227       S004        3  2024-11-01 14:01:51+00:00   \n",
       "6228       S005        3  2024-11-01 14:41:59+00:00   \n",
       "6229       S006        3  2024-11-01 15:41:59+00:00   \n",
       "...         ...      ...                        ...   \n",
       "6988       S008        3  2024-12-31 18:07:55+00:00   \n",
       "6989       S009        3  2024-12-31 19:07:55+00:00   \n",
       "6990       S010        3  2024-12-31 20:07:55+00:00   \n",
       "6991       S011        3  2024-12-31 21:07:55+00:00   \n",
       "6992       S012        3  2024-12-31 21:47:58+00:00   \n",
       "\n",
       "                 EndingDateTime                              Time  \\\n",
       "6225  2024-11-01 13:01:32+00:00  2024-11-01 12:51:36.500000+00:00   \n",
       "6226  2024-11-01 13:41:37+00:00  2024-11-01 13:31:41.500000+00:00   \n",
       "6227  2024-11-01 14:21:42+00:00  2024-11-01 14:11:46.500000+00:00   \n",
       "6228  2024-11-01 15:01:50+00:00  2024-11-01 14:51:54.500000+00:00   \n",
       "6229  2024-11-01 16:01:50+00:00  2024-11-01 15:51:54.500000+00:00   \n",
       "...                         ...                               ...   \n",
       "6988  2024-12-31 18:27:46+00:00  2024-12-31 18:17:50.500000+00:00   \n",
       "6989  2024-12-31 19:27:46+00:00  2024-12-31 19:17:50.500000+00:00   \n",
       "6990  2024-12-31 20:27:46+00:00  2024-12-31 20:17:50.500000+00:00   \n",
       "6991  2024-12-31 21:27:46+00:00  2024-12-31 21:17:50.500000+00:00   \n",
       "6992  2024-12-31 22:07:19+00:00  2024-12-31 21:57:38.500000+00:00   \n",
       "\n",
       "             timestamp_ct date_central  Hour  Minute          time_hr_ct  \\\n",
       "6225  2024-11-01 07:51:36   2024-11-01     7      51 2024-11-01 07:00:00   \n",
       "6226  2024-11-01 08:31:41   2024-11-01     8      31 2024-11-01 08:00:00   \n",
       "6227  2024-11-01 09:11:46   2024-11-01     9      11 2024-11-01 09:00:00   \n",
       "6228  2024-11-01 09:51:54   2024-11-01     9      51 2024-11-01 09:00:00   \n",
       "6229  2024-11-01 10:51:54   2024-11-01    10      51 2024-11-01 10:00:00   \n",
       "...                   ...          ...   ...     ...                 ...   \n",
       "6988  2024-12-31 12:17:50   2024-12-31    12      17 2024-12-31 12:00:00   \n",
       "6989  2024-12-31 13:17:50   2024-12-31    13      17 2024-12-31 13:00:00   \n",
       "6990  2024-12-31 14:17:50   2024-12-31    14      17 2024-12-31 14:00:00   \n",
       "6991  2024-12-31 15:17:50   2024-12-31    15      17 2024-12-31 15:00:00   \n",
       "6992  2024-12-31 15:57:38   2024-12-31    15      57 2024-12-31 15:00:00   \n",
       "\n",
       "     month_year  year  \n",
       "6225    2024-11  2024  \n",
       "6226    2024-11  2024  \n",
       "6227    2024-11  2024  \n",
       "6228    2024-11  2024  \n",
       "6229    2024-11  2024  \n",
       "...         ...   ...  \n",
       "6988    2024-12  2024  \n",
       "6989    2024-12  2024  \n",
       "6990    2024-12  2024  \n",
       "6991    2024-12  2024  \n",
       "6992    2024-12  2024  \n",
       "\n",
       "[768 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7775b098-e49b-4970-b66d-2c4eb6435a24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Completed processing for 2024-11', 'Completed processing for 2024-12')\n"
     ]
    }
   ],
   "source": [
    "# df_2024 = df[df['year']==2024]\n",
    "# date_groups = df_2024.groupby('month_year')\n",
    "\n",
    "# Set up a Dask distributed cluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "# Function to process a single month/year group\n",
    "@delayed\n",
    "def process_group(group_name, group_df):\n",
    "    local_null_data = []\n",
    "    date_list = group_df['date_central'].unique()\n",
    "\n",
    "    for date in date_list:\n",
    "        creds = retrieve_credentials(event)\n",
    "        \n",
    "        client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=creds[\"accessKeyId\"],\n",
    "            aws_secret_access_key=creds[\"secretAccessKey\"],\n",
    "            aws_session_token=creds[\"sessionToken\"]\n",
    "        )\n",
    "        file_df = group_df[group_df['date_central'] == date]\n",
    "        filepaths = list(file_df['FilePath'])\n",
    "        full_time = list(file_df['timestamp_ct'])\n",
    "        daily_xr_list = []\n",
    "        \n",
    "        for i in range(len(filepaths)):\n",
    "            file = filepaths[i]\n",
    "            new_time = str(np.array(full_time[i]))\n",
    "\n",
    "            bucket_name = 'asdc-prod-protected'\n",
    "            object_key = file\n",
    "            local_file_name = f'current_sat_{group_name}.nc'\n",
    "            client.download_file(bucket_name, object_key, local_file_name)\n",
    "\n",
    "            # Processing the NetCDF file\n",
    "            ds = nc.Dataset(local_file_name, mode='r')\n",
    "            support_data_xr = ds.groups['support_data']\n",
    "            support_data_xr = xr.open_dataset(xr.backends.NetCDF4DataStore(support_data_xr))\n",
    "            support_data_xr = support_data_xr[['vertical_column_total', 'eff_cloud_fraction']]\n",
    "            tempo = xr.open_dataset(local_file_name, group=\"/product\")\n",
    "            tempo = xr.merge([tempo, support_data_xr])\n",
    "            lat_lon_coords = xr.open_dataset(local_file_name)\n",
    "            lat = lat_lon_coords.coords['latitude'].values\n",
    "            lon = lat_lon_coords.coords['longitude'].values\n",
    "            tempo = tempo.assign_coords(latitude=(\"latitude\", lat), longitude=(\"longitude\", lon), time=[new_time])\n",
    "\n",
    "            # Filtering and regridding\n",
    "            louisiana_data = tempo.sel(\n",
    "                latitude=(tempo.latitude >= lat_min) & (tempo.latitude <= lat_max),\n",
    "                longitude=(tempo.longitude >= lon_min) & (tempo.longitude <= lon_max)\n",
    "            )\n",
    "            louisiana_data = louisiana_data.where(~(louisiana_data['main_data_quality_flag'] > 1), np.nan)\n",
    "            # lat_new = np.arange(lat_min + 0.005, lat_max, 0.01)\n",
    "            # lon_new = np.arange(lon_min + 0.005, lon_max, 0.01)\n",
    "            # ds_out = xr.Dataset({\"lat\": ([\"lat\"], lat_new), \"lon\": ([\"lon\"], lon_new)})\n",
    "            # regridder = xe.Regridder(louisiana_data, ds_out, \"bilinear\")\n",
    "            # louisiana_data_regrid = regridder(louisiana_data)\n",
    "            daily_xr_list.append(louisiana_data)\n",
    "\n",
    "            # Null count and percentage\n",
    "            null_count = int(louisiana_data.vertical_column_total.isnull().sum().values)\n",
    "            total_elements = louisiana_data.vertical_column_total.size\n",
    "            nan_percentage = round((null_count / total_elements) * 100, 0)\n",
    "            new_row = {'date': date, 'file': file, 'time': new_time, 'real_time': full_time, \n",
    "                       'null_count': null_count, 'percent_null': nan_percentage}\n",
    "            local_null_data.append(new_row)\n",
    "\n",
    "        combined_data = xr.concat(daily_xr_list, dim='time')\n",
    "        combined_data.to_netcdf(f'../../data/tempo_data/no2_daily_files/tempo_{date}.nc')\n",
    "\n",
    "    # Save satellite_null_data for the month/year\n",
    "    local_null_df = pd.DataFrame(local_null_data)\n",
    "    local_null_df.to_csv(f\"../../data/tempo_data/satellite_null_data_raw_{group_name}.csv\", index=False)\n",
    "    return f\"Completed processing for {group_name}\"\n",
    "\n",
    "# Retrieve credentials\n",
    "# creds = retrieve_credentials(event)\n",
    "\n",
    "# Submit tasks to Dask\n",
    "tasks = [process_group(group_name, group_df) for group_name, group_df in date_groups]\n",
    "\n",
    "# Execute tasks\n",
    "results = compute(*tasks)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1ac9b-6593-470b-be0f-e19fb61d96d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Function to process a single month/year group\n",
    "# def process_group(group_name, group_df, creds):\n",
    "#     local_null_data = []\n",
    "#     date_list = group_df['date_central'].unique()\n",
    "\n",
    "#     for date in date_list:\n",
    "#         client = boto3.client(\n",
    "#             's3',\n",
    "#             aws_access_key_id=creds[\"accessKeyId\"],\n",
    "#             aws_secret_access_key=creds[\"secretAccessKey\"],\n",
    "#             aws_session_token=creds[\"sessionToken\"]\n",
    "#         )\n",
    "#         file_df = group_df[group_df['date_central'] == date]\n",
    "#         filepaths = list(file_df['FilePath'])\n",
    "#         full_time = list(file_df['timestamp_ct'])\n",
    "#         daily_xr_list = []\n",
    "\n",
    "#         for i in range(len(filepaths)):\n",
    "#             file = filepaths[i]\n",
    "#             new_time = str(np.array(full_time[i]))\n",
    "\n",
    "#             bucket_name = 'asdc-prod-protected'\n",
    "#             object_key = file\n",
    "#             local_file_name = f'current_sat_{group_name}.nc'\n",
    "#             client.download_file(bucket_name, object_key, local_file_name)\n",
    "\n",
    "#             # Processing the NetCDF file\n",
    "#             ds = nc.Dataset(local_file_name, mode='r')\n",
    "#             support_data_xr = ds.groups['support_data']\n",
    "#             support_data_xr = xr.open_dataset(xr.backends.NetCDF4DataStore(support_data_xr))\n",
    "#             support_data_xr = support_data_xr[['vertical_column_total', 'eff_cloud_fraction', 'surface_pressure']]\n",
    "#             tempo = xr.open_dataset(local_file_name, group=\"/product\")\n",
    "#             tempo = xr.merge([tempo, support_data_xr])\n",
    "#             lat_lon_coords = xr.open_dataset(local_file_name)\n",
    "#             lat = lat_lon_coords.coords['latitude'].values\n",
    "#             lon = lat_lon_coords.coords['longitude'].values\n",
    "#             tempo = tempo.assign_coords(latitude=(\"latitude\", lat), longitude=(\"longitude\", lon), time=[new_time])\n",
    "\n",
    "#             # Filtering and regridding\n",
    "#             louisiana_data = tempo.sel(\n",
    "#                 latitude=(tempo.latitude >= lat_min) & (tempo.latitude <= lat_max),\n",
    "#                 longitude=(tempo.longitude >= lon_min) & (tempo.longitude <= lon_max)\n",
    "#             )\n",
    "#             louisiana_data = louisiana_data.where(~(louisiana_data['main_data_quality_flag'] > 1), np.nan)\n",
    "           \n",
    "#             lat_new = np.arange(lat_min + 0.005, lat_max, 0.01)\n",
    "#             lon_new = np.arange(lon_min + 0.005, lon_max, 0.01)\n",
    "#             ds_out = xr.Dataset({\"lat\": ([\"lat\"], lat_new), \"lon\": ([\"lon\"], lon_new)})\n",
    "#             if louisiana_data.dims:\n",
    "#                 regridder = xe.Regridder(louisiana_data, ds_out, \"bilinear\")\n",
    "#                 louisiana_data_regrid = regridder(louisiana_data)\n",
    "#                 daily_xr_list.append(louisiana_data_regrid)\n",
    "\n",
    "#                 # Null count and percentage\n",
    "#                 null_count = int(louisiana_data_regrid.vertical_column_total.isnull().sum().values)\n",
    "#                 total_elements = louisiana_data_regrid.vertical_column_total.size\n",
    "#                 nan_percentage = round((null_count / total_elements) * 100, 0)\n",
    "#                 new_row = {'date': date, 'file': file, 'time': new_time, 'real_time': full_time, \n",
    "#                            'null_count': null_count, 'percent_null': nan_percentage}\n",
    "#                 local_null_data.append(new_row)\n",
    "            \n",
    "#             else:\n",
    "#                 print(f\"Skipping regridding for date: {date} as data is empty!\")\n",
    "#                 continue\n",
    "\n",
    "#         combined_data = xr.concat(daily_xr_list, dim='time')\n",
    "#         combined_data.to_netcdf(f'../../data/tempo_data/no2_daily_files/tempo_{date}.nc')\n",
    "\n",
    "#     # Save satellite_null_data for the month/year\n",
    "#     local_null_df = pd.DataFrame(local_null_data)\n",
    "#     local_null_df.to_csv(f\"../../data/tempo_data/satellite_null_data_{group_name}.csv\", index=False)\n",
    "\n",
    "# # Main thread management\n",
    "# def process_all_groups():\n",
    "#     creds = retrieve_credentials(event)  # Retrieve credentials once\n",
    "#     threads = []\n",
    "\n",
    "#     for group_name, group_df in date_groups:\n",
    "#         thread = Thread(target=process_group, args=(group_name, group_df, creds))\n",
    "#         threads.append(thread)\n",
    "#         thread.start()\n",
    "\n",
    "#     # Wait for all threads to complete\n",
    "#     for thread in threads:\n",
    "#         thread.join()\n",
    "\n",
    "# process_all_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43896aff-197d-48ee-b5f6-33e91b7576b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### TIME 5 days = 875 s (~15 min)\n",
    "# # Initialize the Google Cloud Storage client\n",
    "# # Example DataFrame (assuming it exists)\n",
    "# satellite_null_data = pd.DataFrame({\n",
    "#     'date': [],\n",
    "#     'file': [],\n",
    "#     'time': [],\n",
    "#     'null_count': [],\n",
    "#     'percent_null': []\n",
    "# })\n",
    "\n",
    "# date_list = list(df['date_central'].unique())\n",
    "\n",
    "# for date in date_list:\n",
    "#     # Retrieve credentials\n",
    "#     creds = retrieve_credentials(event)\n",
    "\n",
    "#     # Use the credentials to access the S3 bucket\n",
    "#     client = boto3.client('s3',\n",
    "#         aws_access_key_id=creds[\"accessKeyId\"],\n",
    "#         aws_secret_access_key=creds[\"secretAccessKey\"],\n",
    "#         aws_session_token=creds[\"sessionToken\"]\n",
    "#     )\n",
    "    \n",
    "#     file_df = df[df['date_central']==date]\n",
    "#     filepaths = list(file_df['FilePath'])\n",
    "#     # times = list(file_df['time_hr_ct'])\n",
    "#     full_time = list(file_df['timestamp_ct'])\n",
    "    \n",
    "#     daily_xr_list = []\n",
    "    \n",
    "#     for i in range(len(filepaths)):\n",
    "#         file=filepaths[i]\n",
    "#         new_time = str(np.array(full_time[i]))\n",
    "        \n",
    "#         bucket_name = 'asdc-prod-protected'\n",
    "#         object_key = file\n",
    "#         local_file_name = 'current_sat.nc'\n",
    "#         client.download_file(bucket_name, object_key, local_file_name)\n",
    "        \n",
    "#         ds = nc.Dataset('current_sat.nc', mode='r')\n",
    "#         support_data_xr = ds.groups['support_data']\n",
    "#         support_data_xr = xr.open_dataset(xr.backends.NetCDF4DataStore(support_data_xr))\n",
    "#         support_data_xr=support_data_xr[['vertical_column_total','eff_cloud_fraction']]\n",
    "#         tempo = xr.open_dataset('current_sat.nc', group=\"/product\")\n",
    "#         tempo = xr.merge([tempo, support_data_xr])\n",
    "#         del ds\n",
    "#         del support_data_xr\n",
    "#         lat_lon_coords = xr.open_dataset('current_sat.nc')\n",
    "#         lat = lat_lon_coords.coords['latitude'].values\n",
    "#         lon = lat_lon_coords.coords['longitude'].values\n",
    "#         tempo = tempo.assign_coords(latitude=(\"latitude\", lat), longitude=(\"longitude\", lon), time=[new_time])\n",
    "#         del lat\n",
    "#         del lon\n",
    "\n",
    "#         # Filter the data using the .sel method for selecting ranges in xarray\n",
    "#         louisiana_data = tempo.sel(latitude=(tempo.latitude  >= lat_min) & (tempo.latitude <= lat_max))\n",
    "#         louisiana_data = louisiana_data.sel(longitude=(tempo.longitude  >= lon_min) & (tempo.longitude <= lon_max))\n",
    "#         del tempo\n",
    "        \n",
    "#         mask = louisiana_data['main_data_quality_flag'] >1 \n",
    "#         louisiana_data = louisiana_data.where(~mask, other=np.nan)\n",
    "#         mask = louisiana_data['main_data_quality_flag'].isnull()\n",
    "#         louisiana_data = louisiana_data.where(~mask, other=np.nan)\n",
    "        \n",
    "#         lat_new = np.arange(lat_min+0.005, lat_max, 0.01)\n",
    "#         lon_new = np.arange(lon_min+0.005, lon_max, 0.01)\n",
    "\n",
    "#         ds_out = xr.Dataset(\n",
    "#             {\n",
    "#                 \"lat\": ([\"lat\"], lat_new),\n",
    "#                 \"lon\": ([\"lon\"], lon_new),\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#         # Create the regridder object.\n",
    "#         regridder = xe.Regridder(louisiana_data, ds_out, \"bilinear\")\n",
    "\n",
    "#         # Apply the regridding operation.\n",
    "#         louisiana_data_regrid = regridder(louisiana_data)\n",
    "#         daily_xr_list.append(louisiana_data_regrid)\n",
    "        \n",
    "#         null_count = int(louisiana_data_regrid.vertical_column_troposphere.isnull().sum().values)\n",
    "#         total_elements = louisiana_data_regrid.vertical_column_troposphere.size\n",
    "#         nan_percentage = round((null_count / total_elements) * 100,0)\n",
    "        \n",
    "#         new_row = {'date':date, 'file':file, 'time':new_time, 'real_time':full_time, 'null_count':null_count, 'percent_null':nan_percentage}\n",
    "#         # Convert the new row to a DataFrame\n",
    "#         new_row_df = pd.DataFrame([new_row])\n",
    "#         # Append the new row to the DataFrame using concat\n",
    "#         satellite_null_data = pd.concat([satellite_null_data, new_row_df], ignore_index=True)\n",
    "        \n",
    "#     combined_data = xr.concat(daily_xr_list, dim='time')\n",
    "    \n",
    "#     combined_data.to_netcdf(f'../../data/tempo_data/daily_files/tempo_{date}.nc')\n",
    "         \n",
    "\n",
    "# satellite_null_data.to_csv(\"../../data/tempo_data/satellite_null_data_09_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff40249-6917-4537-8420-91da9847a3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # Open the file with xarray\n",
    "# ds = xr.open_dataset('daily.nc')\n",
    "# # Assuming your xarray is called 'x'\n",
    "# # Define the bounding box for Louisiana\n",
    "# lat_min, lat_max = 28.92, 33.02\n",
    "# lon_min, lon_max = -94.04, -88.82\n",
    "\n",
    "# subset = ds.sel(time='2023-08-07 07:00:00')\n",
    "\n",
    "# # Plot the weight variable in Louisiana's bounding box\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# subset.vertical_column_troposphere.plot(cmap='viridis')\n",
    "# plt.title('Weight Variable in Louisiana Bounding Box')\n",
    "# plt.xlabel('Longitude')\n",
    "# plt.ylabel('Latitude')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "ds_env",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python (ds_env) (Local)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
